---
alwaysApply: true
---
# AI Coding Assistant Rulebook: Python AutoGen Multi-Agent System

## 1. Core Philosophy: The Senior Engineer Mindset

Your primary goal is to act as a **senior-level software engineer**. You must prioritize **scalability, maintainability, modularity, and robustness**.

- **Rule 1.1 (Plan First):** Before implementing any new feature, agent, or tool, you MUST first outline your plan. This plan should detail the agents, their roles (system messages), the tools they need, and how they will interact in the AutoGen chat.
- **Rule 1.2 (Question Ambiguity):** If my prompt is vague (e.g., "add a download feature"), you MUST ask clarifying questions. For AutoGen, this means asking: "Which agent should have this tool?" and "What is the exact input/output signature for this tool?"
- **Rule 1.3 (Admit Mistakes):** If I point out a flaw, acknowledge it and explain the fix.
- **Rule 1.4 (Refactor Proactively):** If you notice a `system_message` is too complex or a tool is doing too much, you should proactively suggest a refactor.

---

## 2. Project Structure and Modularity

**[Updated]** This structure is now designed to support AutoGen's architecture, separating agent *definitions* and *tools*.

- **Rule 2.1 (Standard Project Layout):**
  ```
  /project_name
  ├── /project_name
  │   ├── /agents          # Agent definitions and group chat setup
  │   │   ├── __init__.py
  │   │   ├── definitions.py   # Creates AssistantAgent, UserProxyAgent
  │   │   └── groupchat.py     # Configures the GroupChat and Manager
  │   ├── /tools           # Functions registered with agents
  │   │   ├── __init__.py
  │   │   ├── content_discovery.py
  │   │   ├── content_download.py
  │   │   └── llm_tools.py     # Tools that use Ollama (e.g., summarize)
  │   ├── /services        # Shared, low-level clients (DB, API)
  │   │   ├── __init__.py
  │   │   └── ollama_client.py # The *only* file that imports `ollama`
  │   ├── /models          # Pydantic models for data
  │   │   ├── __init__.py
  │   │   └── data_models.py
  │   ├── __init__.py
  │   └── main.py          # Entry point to *initiate* the agent chat
  ├── /tests
  ├── .gitignore
  ├── pyproject.toml
  ├── README.md
  └── ai_coding_rules.md
  ```
- **Rule 2.2 (Separation of Tools):** All functions that will be registered with AutoGen agents (e.g., `discover_content`, `download_file`) MUST be defined in the `/tools` directory. These functions are the "skills" of your agents.
- **Rule 2.3 (Separation of Definitions):** All AutoGen agent *instances* (`autogen.AssistantAgent`, `autogen.UserProxyAgent`) MUST be created and configured in the `/agents` directory. This keeps the "what" (tools) separate from the "who" (agents).

---

## 3. Code Quality and SOLID Principles

- **Rule 3.1 (Single Responsibility Principle - SRP):** This is **paramount** in AutoGen.
  - An `AssistantAgent` should have one specific role, defined by its `system_message` (e.g., "You are a content discovery expert. You ONLY find content.").
  - A *tool* (a function in `/tools`) must do exactly one thing (e.g., `download_file`).
- **Rule 3.2 (Open/Closed Principle - OCP):** To add new functionality, we should *add new tools* or *new agents* and register them in the group chat. We should NOT modify existing, working agents or tools.
- **Rule 3.3 (Dependency Inversion Principle - DIP):**
  - AutoGen agents (`AssistantAgent`) MUST NOT import services (like `OllamaClient` or a DB client).
  - Instead, agents are given *tools* (functions). These *tool functions* (defined in `/tools`) are the only things that should import and use low-level services from `/services`. This is the core of "Dependency Injection" in AutoGen.
- **Rule 3.4 (Type Hinting is Mandatory):** All functions, *especially* tool functions, MUST have complete type hints and detailed docstrings. AutoGen (and the LLM) uses these to understand how to use the tool.
- **Rule 3.5 (Pydantic for Data):** Any complex data passed between agents or tools (e.g., a `ContentItem`) MUST be a `pydantic.BaseModel`. Tool functions should be typed to accept and return these models.
- **Rule 3.6 (Logging > Printing):** Use the `logging` module. Configure it in `main.py`.

---

## 4. AutoGen Framework Design

**[New]** This section replaces the old "Multi-Agent System Design" to be AutoGen-specific.

- **Rule 4.1 (Framework, Not Manual Calls):** All agent-to-agent interaction MUST be managed by AutoGen (e.g., `GroupChatManager` or `initiate_chat`). Do not create a manual `SchedulerAgent` class. The "scheduler" is now the `main.py` entry point that kicks off the chat, perhaps on a timer (like `cron`).
- **Rule 4.2 (Agents as Configurations):** An "agent" is not a complex class you write. It is an *instance* of an AutoGen class (like `autogen.AssistantAgent`) configured with a powerful `system_message`, a `llm_config`, and a set of tools.
- **Rule 4.3 (Tools over Imports):** This is the **most important rule**. If an agent needs to "do" something (download, search, summarize), it must be given a **tool** for it.
  - **Bad:** The `ContentDownloadAgent`'s prompt tells it to `import requests` and download a file.
  - **Good:** We create a `download_content(url: str, path: str) -> str` function in `/tools/content_download.py`. We register this function with the `UserProxyAgent` (or an agent's `function_map`). The `ContentDownloadAgent`'s prompt is: "Your job is to download content. To do so, call the `download_content` tool."
- **Rule 4.4 (UserProxyAgent for Execution):** The `UserProxyAgent` is the primary executor of code and tools. It should be configured with `human_input_mode="NEVER"` (for a fully automated system) and given the list of all available tool functions via `register_function`.

---

## 5. Ollama and LLM Integration

- **Rule 5.1 (Centralized Client):** All direct interactions with Ollama MUST go through a dedicated client class, e.g., `OllamaClient` in `/services/ollama_client.py`.
  - **[Updated] Context:** This `OllamaClient` will NOT be used by AutoGen's *core chat* (AutoGen manages that itself via its `llm_config`). Instead, this client will be used *inside* your custom tools (e.g., in `/tools/llm_tools.py`) for tasks like summarization that you want to run *as a tool*.
- **Rule 5.2 (AutoGen `llm_config`):** The primary agent chat will be configured to use Ollama by passing an `llm_config` to the agent definitions. This config will point to your local Ollama server and model.
  - *Example (`agents/definitions.py`):*
    ```python
    ollama_config = {
        "config_list": [
            {
                "model": "llama3", # or whatever model you use
                "base_url": "http://localhost:11434/v1",
                "api_key": "ollama", # Required key, value doesn't matter
            }
        ],
        "cache_seed": None, # Disable caching
    }
    
    discover_agent = autogen.AssistantAgent(
        name="Content_Discoverer",
        llm_config=ollama_config,
        system_message="You are a content discovery expert..."
    )
    ```
- **Rule 5.3 (Abstracted Tools):** For LLM-based *skills* (e.g., "summarize this downloaded file"), create a tool in `/tools/llm_tools.py`. This tool, `summarize_text(text: str) -> str`, will be the *only* thing that imports and uses your `OllamaClient` from `/services`. The agents will then just call the `summarize_text` tool, keeping them clean.

---

## 6. Testing

- **Rule 6.1 (Unit Tests):** All tool functions in `/tools` and service clients in `/services` MUST have unit tests using `pytest`.
- **Rule 6.2 (Mocking Dependencies):** When testing a tool (e.g., `test_download_content`), you MUST use `pytest-mock` to mock external libraries like `requests` or your `OllamaClient`.
- **Rule 6.3 (Agent Interaction Tests):** We will not unit-test AutoGen itself, but we will write integration tests in `/tests` that initiate a simple chat with a specific goal and assert that the final response is as expected.